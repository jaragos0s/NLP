####################
# Updated : 2022.07.04.
####################
# Seunghui Choi
####################
# Crawling the number of articles in Google News Tab (Daily)
####################

import sys
import os
from xml.dom.pulldom import START_ELEMENT
import numpy as np
import pandas as pd
import requests
import time
from datetime import date, datetime
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By

def scrape(
    query,
    start,
    end
):

    
    news_dict = {}
    for idx, dt in enumerate(pd.date_range(start, end)):
        dt = datetime.strftime(dt, '%Y%m%d')
        start_y, start_m, start_d = dt[:4], dt[4:6].lstrip("0"), dt[6:8].lstrip("0")
        time.sleep(1)
        options = webdriver.ChromeOptions()
        options.add_argument('headless')
        options.add_argument('window-size=1920x937')
        options.add_argument('disable-gpu')
        chrome_path = 'c:/chromedriver_win32/chromedriver.exe'  #### Input your own Chrome path
        browser = webdriver.Chrome(chrome_path, chrome_options=options)
        sxsrf = #### Input your own sxsrf
        url = 'https://www.google.com/search?q={}&biw=1920&bih=937&sxsrf={}&source=lnt&tbs=cdr%3A1%2Ccd_min%3A{}%2F{}%2F{}%2Ccd_max%3A{}%2F{}%2F{}&tbm=nws'.format(query,sxsrf, start_m, start_d, start_y,start_m, start_d, start_y)
        browser.get(url)
        headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'}
        req = requests.get(url, headers = headers)
        soup = BeautifulSoup(req.text, 'html.parser')
    
        txt = soup.find('div', {'id' : 'result-stats'})
        if txt:
            news_dict[idx] =  {'start_m' : start_m, 'start_d' : start_d, 'start_y' : start_y, 'end_m' : start_m, 'end_d': start_d, 'end_y' : start_y, 'txt' : txt.text}
        else:
            news_dict[idx] =  {'start_m' : start_m, 'start_d' : start_d, 'start_y' : start_y, 'end_m' : start_m, 'end_d': start_d, 'end_y' : start_y, 'txt' : None}

    return news_dict

def main(query,
        start,
        end
        ):

    #### 데이터프레임 변환 ####
    
    scraps = scrape(query,
                    start,
                    end
                    )
    print('데이터프레임 변환\n')
    news_df = pd.DataFrame(scraps).T
    news_df.to_csv('{}_{}.csv'.format(query, start))



if __name__ == "__main__":
    ## parameters
    #### Topic that you want to search
    query = "Cyclone" 
    #### Start date
    start = '2005-01-01'
    #### End date
    end = '2005-12-31'
    main(
        query,
        start,
        end
    )


