{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aa8e856",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tokenization_kobert'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-3a9b631be0b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtokenization_kobert\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKoBertTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tokenization_kobert'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tokenization_kobert import KoBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "74deb562",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = nn.Parameter(torch.empty(d_model, d_ff))\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.encoding)):\n",
    "            for j in range(len(self.encoding[0])):\n",
    "                if j % 2 == 0:\n",
    "                    self.encoding[i][j] = torch.sin(i / 10000**(j / d_model))\n",
    "                else:\n",
    "                    self.encoding[i][j] = torch.cos(i / 10000**((j - 1) / d_model))\n",
    "        return self.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6b986d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcess(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PreProcess, self).__init__()\n",
    "        self.encoding = PositionalEncoding(d_model, d_ff)\n",
    "        self.embedding = nn.Embedding(d_ff, d_model)\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x += self.encoding(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c1e44dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(*params, init_fnc=None):\n",
    "    for param in params:\n",
    "        if init_fnc is not None:\n",
    "            init_fnc(param)\n",
    "        else:\n",
    "            torch.nn.init.normal_(param)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model, d_k=None, d_v=None):\n",
    "        super(Attention, self).__init__()\n",
    "        if d_k is None:\n",
    "            d_k = d_model\n",
    "        if d_v is None:\n",
    "            d_v = d_model\n",
    "            \n",
    "        self.W_k = nn.Parameter(torch.empty(d_model, d_k))\n",
    "        self.W_q = nn.Parameter(torch.empty(d_model, d_k))\n",
    "        self.W_v = nn.Parameter(torch.empty(d_model, d_v))\n",
    "        self.dot_scale = d_model ** .5\n",
    "        init_params(self.W_k, self.W_q, self.W_v)\n",
    "        \n",
    "    def forward(self, *, query, key, value):\n",
    "        Q = query.matmul(self.W_q)\n",
    "        K = key.matmul(self.W_k)\n",
    "        V = value.matmul(self.W_v)\n",
    "        A = F.softmax(Q.matmul(K.transpose(1, 2)) / self.dot_scale, dim=1)\n",
    "        return A.matmul(V)\n",
    "\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_k=None, d_v=None):\n",
    "        super(MHA, self).__init__()\n",
    "        if d_k is None:\n",
    "            d_k = d_model // n_heads\n",
    "            \n",
    "        if d_v is None:\n",
    "            d_v = d_model // n_heads\n",
    "\n",
    "        self.heads = nn.ModuleList([Attention(d_model=d_model, d_k=d_k, d_v=d_v) for _ in range(n_heads)])\n",
    "        self.W_o = nn.Parameter(torch.empty(d_v * n_heads, d_model))\n",
    "        init_params(self.W_o)\n",
    "        \n",
    "    def forward(self, query, key, value):\n",
    "        As = []\n",
    "        for head in self.heads:\n",
    "            As.append(head(\n",
    "                query=query,\n",
    "                key=key,\n",
    "                value=value,\n",
    "            ))\n",
    "        As = torch.cat(As, dim=-1)\n",
    "        return As.matmul(self.W_o)\n",
    "\n",
    "\n",
    "class MHACombined(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_k=None, d_v=None, masking = False):\n",
    "        super(MHACombined, self).__init__()\n",
    "        if d_k is None:\n",
    "            d_k = d_model // n_heads\n",
    "        if d_v is None:\n",
    "            d_v = d_model // n_heads\n",
    "        self.masking = masking    \n",
    "        self.W_k = nn.Parameter(torch.empty(n_heads, d_model, d_k))\n",
    "        self.W_q = nn.Parameter(torch.empty(n_heads, d_model, d_k))\n",
    "        self.W_v = nn.Parameter(torch.empty(n_heads, d_model, d_v))\n",
    "        self.dot_scale = d_model ** .5\n",
    "        self.W_o = nn.Parameter(torch.empty(d_v * n_heads, d_model))\n",
    "        init_params(self.W_k, self.W_q, self.W_v, self.W_o)\n",
    "        \n",
    "    def forward(self, query, key, value):\n",
    "        Q = query.unsqueeze(1).matmul(self.W_q)\n",
    "        K = key.unsqueeze(1).matmul(self.W_k)\n",
    "        V = value.unsqueeze(1).matmul(self.W_v)\n",
    "        QK = Q.matmul(K.transpose(-2, -1)) / self.dot_scale\n",
    "        if self.masking is True:\n",
    "            mask = torch.full_like(QK, -float('Inf'))\n",
    "            mask = mask.triu(1)\n",
    "            QK += mask\n",
    "        QK = torch.nan_to_num(F.softmax(QK, dim=-1))\n",
    "        A_ = QK.matmul(V)\n",
    "        A = A_.permute(0, 2, 1, 3).reshape(A_.shape[0], A_.shape[2], -1)        \n",
    "\n",
    "        return A.matmul(self.W_o)\n",
    "\n",
    "\n",
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, activation=None):\n",
    "        super(FullyConnected, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            activation if activation is not None else nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, d_k=None, d_v=None, n_heads=8):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "#         self.attention = MHA(n_heads=n_heads, d_model=d_model, d_k=d_k, d_v=d_v)\n",
    "        self.attention = MHACombined(n_heads=n_heads, d_model=d_model, d_k=d_k, d_v=d_v)\n",
    "        self.fc = FullyConnected(d_model, d_ff)\n",
    "        self.norm_att = nn.LayerNorm(d_model)\n",
    "        self.norm_fc = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x += self.attention(x, x, x)\n",
    "        x = self.norm_att(x)\n",
    "        x += self.fc(x)\n",
    "        x = self.norm_fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, n_heads=8, n_layers=6, d_v=None, d_k=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([EncoderLayer(\n",
    "            d_model=d_model,\n",
    "            d_ff=d_ff,\n",
    "            n_heads=n_heads,\n",
    "            d_v=d_v,\n",
    "            d_k=d_k,\n",
    "        ) for _ in range(n_layers)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x += layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e7f26547",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, d_k = None, d_v = None, n_heads = 8):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.masked_multi_head = MHACombined(n_heads = n_heads, d_model = d_model, d_k = d_k, d_v = d_v, masking = True)\n",
    "        self.multi_head = MHACombined(n_heads = n_heads, d_model = d_model, d_k = d_k, d_v = d_v, masking = False)\n",
    "        self.fc = FullyConnected(d_model, d_ff)\n",
    "        self.norm_masked_att = nn.LayerNorm(d_model)\n",
    "        self.norm_att = nn.LayerNorm(d_model)\n",
    "        self.norm_fc = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x, encoder_out):\n",
    "        x += self.masked_multi_head(x, x, x)\n",
    "        x = self.norm_masked_att(x)\n",
    "        \n",
    "        x += self.multi_head(encoder_out, encoder_out, x)\n",
    "        x = self.norm_att(x)\n",
    "        \n",
    "        x += self.fc(x)\n",
    "        x = self.norm_fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, n_heads = 8, n_layers = 6, d_v = None, d_k = None):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([DecoderLayer(\n",
    "            d_model = d_model,\n",
    "            d_ff = d_ff,\n",
    "            n_heads = n_heads,\n",
    "            d_v = d_v,\n",
    "            d_k = d_k        \n",
    "        ) for _ in range(n_layers)])\n",
    "        \n",
    "    def forward(self, x, encoder_out):\n",
    "        for layer in self.layers:\n",
    "            x += layer(x, encoder_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9530c0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Output(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(Output, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn. Softmax(dim = 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d39f9138",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "d_ff = 10000\n",
    "n_heads = 8\n",
    "n_layers = 6\n",
    "\n",
    "preprocess = PreProcess(d_model, d_ff)\n",
    "encoder = Encoder(d_model, d_ff, n_heads, n_layers)\n",
    "decoder = Decoder(d_model, d_ff, n_heads, n_layers)\n",
    "output = Output(d_model, d_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e10c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./ratings_train.txt', delimiter='\\t', index_col='id')\n",
    "x_train = df_train['document'].values[:100]\n",
    "inp = tokenizer.batch_encode_plus(\n",
    "    x_train.astype('U'),\n",
    "    padding=True, \n",
    "    return_tensors='pt'\n",
    ")['input_ids']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7b9fb876",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-b8e416700b21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'안녕하세요'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'저는'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'최승희'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'입니다'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "out = preprocess(inp)\n",
    "print(out)\n",
    "out = encoder(out)\n",
    "print(out)\n",
    "out = decoder(out)\n",
    "print(out)\n",
    "out = Output(out)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91904f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
