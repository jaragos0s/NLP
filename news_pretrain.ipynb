{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fcce879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import torchvision.models as models\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import GPT2LMHeadModel\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50cb40fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    # 0. None 기사 삭제\n",
    "    df = df.dropna()\n",
    "    # 1. 중복 기사 삭제\n",
    "    df = df.drop_duplicates('url', keep = 'first')\n",
    "    \n",
    "    # 2. text에 있는 동영상 뉴스 삭제\n",
    "    df['text'] = df['text'].replace('동영상 뉴스', '')\n",
    "    \n",
    "    # 3. [str] format 삭제\n",
    "    regex_search_term = '(\\[[^(\\[|\\]);]*\\])+'\n",
    "    regex_replacement = ''\n",
    "    df['text'] = [re.sub(regex_search_term, '', string) for string in df['text']]\n",
    "    \n",
    "    # 4. (##=연합뉴스) 삭제\n",
    "    regex_search_term = '\\(((.*)=연합뉴스)\\)+'\n",
    "    df['text'] = [re.sub(regex_search_term, '', string) for string in df['text']]\n",
    "\n",
    "    # 5. post_date 날짜 처리\n",
    "    df['post_date'] = df['post_date'].replace('기사입력', '')\n",
    "    # df['post_date'] = df['post_date'].split('최종수정')[0]\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fff43c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, data_files):\n",
    "        tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "            \"skt/kogpt2-base-v2\",\n",
    "            bos_token='</s>', \n",
    "            eos_token='</s>', \n",
    "            unk_token='<unk>',\n",
    "            pad_token='<pad>', \n",
    "            mask_token='<mask>'\n",
    "        )\n",
    "        df_train = pd.DataFrame()\n",
    "        for file in data_files:\n",
    "            data = pd.read_csv(file, encoding = 'cp949')\n",
    "            df_train = pd.concat([df_train, data[:5]])\n",
    "            \n",
    "            # create text txt.file \n",
    "            f = open('{}_text.txt'.format(file.split('.')[0]), mode = 'wt', encoding = 'utf-8')\n",
    "            f.write(data['text'].to_string())\n",
    "            f.close()\n",
    "            df_train = preprocessing(df_train)\n",
    "        self.x_train = tokenizer.batch_encode_plus(df_train['text'].to_list(), padding = True, return_tensors = \"pt\")['input_ids']\n",
    "        self.y_train = self.x_train[1:]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_train)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.x_train[idx]\n",
    "        label = self.y_train[idx]\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6aa5f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y):\n",
    "    return (y_pred == y).sum() / (len(y) - 1)\n",
    "\n",
    "def training(inputs, epochs, batch_size):\n",
    "    losses, acces = [], []\n",
    "    model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "    \n",
    "    base_opt = torch.optim.SGD(model.parameters(), lr = 0.1)\n",
    "    optimizer = torchcontrib.optim.SWA(base_opt, swa_start = 10, swa_freq = 5, swa_lr = 0.05)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        m_losses, m_acces = [], []\n",
    "        for batch in range(batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(inputs, labels = inputs)\n",
    "        \n",
    "            loss = model.loss  # cross entropy loss\n",
    "            losses.append(loss)\n",
    "        \n",
    "            acc = accuracy(outputs, inputs[1:])\n",
    "            acces.append(acc)\n",
    "        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            if epoch > 0 and epoch % 5 == 0:\n",
    "                optimizer.update_swa()\n",
    "                print('epoch : {3.} / {} batch_size : {3.} / {} ... train loss : {}\\tacc : {}'.format(epoch, epochs, batch, batch_size, m_losses.mean(), m_acces.mean()))\n",
    "                m_losses, m_acces = [], []\n",
    "            \n",
    "        \n",
    "    optimizer.swap_swa_sgd()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "989bb026",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "data_files = [\"실종.csv\", \"대피.csv\", \"정전.csv\", \"가뭄.csv\", \"지진.csv\", \"코로나.csv\", \"테러.csv\", \"홍수.csv\", \"화재.csv\"]\n",
    "dataset = NewsDataset(data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0023af",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b5cc355",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "inputs = dataset.x_train\n",
    "print(inputs)\n",
    "outputs = model(inputs[:, :512], labels = inputs[:, :512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3465077f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.2250, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d975d8a",
   "metadata": {},
   "source": [
    "### 연습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bf2ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = '안녕하세요 (서울 = 연합뉴스)입니다 (전주 = 연합뉴스) 이기도 하고요'\n",
    "txt = txt.replace('%s = 연합뉴스)' % format('전주'), '')\n",
    "txt = txt.split('입니다')\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c433f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame({'c1': ['안녕', '하세요', '안녕'], 'c2' : ['hi', 'hello', 'my']})\n",
    "a = a.drop_duplicates('c1', keep = 'first')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5579771e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b86936",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_search_term = '(\\[[^(\\[|\\]);]*\\])+'\n",
    "regex_replacement = ''\n",
    "text_before = ['[연합뉴스] 안녕하세요[] [승희] 하이[dlfkdl]fdskf;k;s', '어디가세요']\n",
    "for text in text_before:\n",
    "    text_after = re.sub(regex_search_term, regex_replacement, text)\n",
    "    print((text_after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2115a509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex_term = '(\\(*=연합뉴스)\\)+'\n",
    "regex_term = '\\(((.*)=연합뉴스)\\)+'\n",
    "text = '(서울=연합뉴스) 한정애 환경부 장관'\n",
    "# text2 = '(전주=연합뉴스) 승희'\n",
    "text_after = re.sub(regex_term, '', text)\n",
    "text_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "109648cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
    "  bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
    "  pad_token='<pad>', mask_token='<mask>') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164f682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '근육이 커지기 위해서는'\n",
    "input_ids = tokenizer.encode(text)\n",
    "en_ids = model.generate(torch.tensor([input_ids]),\n",
    "                           max_length=128,\n",
    "                           repetition_penalty=2.0,\n",
    "                           pad_token_id=tokenizer.pad_token_id,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           bos_token_id=tokenizer.bos_token_id,\n",
    "                           use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44beb61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.tensor([input_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "476cd3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10553, 14308, 48893, 11849, 463, 10477, 13038, 739, 9969, 11925, 459, 12895, 739, 51200], [10553, 14308, 48893, 11849, 463, 11925, 10272, 739, 9969, 11925, 459, 12895, 739, 51200]]\n",
      "[13, 13]\n",
      "tensor([[[10553, 14308, 48893, 11849,   463, 10477, 13038,   739,  9969, 11925,\n",
      "            459, 12895,   739, 51200],\n",
      "         [10553, 14308, 48893, 11849,   463, 11925, 10272,   739,  9969, 11925,\n",
      "            459, 12895,   739, 51200]]])\n",
      "tensor([[13, 13]])\n"
     ]
    }
   ],
   "source": [
    "choices = [\"Hello, my dog is cute [CLS]\", \"Hello, my cat is cute [CLS]\"]\n",
    "num_added_tokens = tokenizer.add_special_tokens({'cls_token': '[CLS]'})\n",
    "encoded_choices = [tokenizer.encode(s) for s in choices]\n",
    "print(encoded_choices)\n",
    "cls_token_location = [tokens.index(tokenizer.cls_token_id) for tokens in encoded_choices]\n",
    "print(cls_token_location)\n",
    "input_ids = torch.tensor(encoded_choices).unsqueeze(0)  # Batch size: 1, number of choices: 2\n",
    "print(input_ids)\n",
    "mc_token_ids = torch.tensor([cls_token_location])  # Batch size: 1\n",
    "print(mc_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2110d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
